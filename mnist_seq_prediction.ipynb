{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "mnist_seq_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c957f179"
      },
      "source": [
        "## Part II: CNN-LSTM for Sequence Prediction\n",
        "### Introduction:\n",
        "\n",
        "- In this section, you'll extend the CNN from section (1) to a hybrid CNN-LSTM model to predict the future elements of a sequence.\n",
        "- Instead of providing a single digit image to predict its category, you will be given a sequence of digit images. These sequences follow a pattern in that each consecutive image pair will be shifted by some constant amount. You will design a CNN-LSTM model to recognize those patterns from raw images and predict the future digits.\n",
        "- For instance, the input and target of the CNN-LSTM model can be the image squence whose categories are shown as below: \n",
        "            Input: 1,3,5,7,9,1   Target: 3,5,7  (shifted by 2)\n",
        "            Input: 2,6,0,4,8,2   Target: 6,0,4  (shifted by 4)\n",
        "where the input and target consists of 5 elements and 3 elements, respectively.\n",
        "- The training sequence are generated by varying shifts. We also include some unseen shifts in test sequences to validate the model's generalization ability.\n",
        "\n",
        "### Task:\n",
        "- You need to design the model and complete the training loop with Pytorch.\n",
        "- You need to achieve 93% averaged Top1 Acc on test data.\n",
        "- This experiment shares the same dataset with the first section. Once you prepare the data following the first section, you do not need to download extra content. \n"
      ],
      "id": "c957f179"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQBwQEobozZg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "01aa4b65-3a42-4088-fea1-feed1a3d171b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "KQBwQEobozZg",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14e6af01"
      },
      "source": [
        "# The arguments of the expeirment\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        # Based on the availablity of GPU, decide whether to run the experiment on cuda or cpu.\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # The random seed for the exp.\n",
        "        self.seed = 1\n",
        "        # The mini batch size of training and testing data. If you find you machines run very slow \n",
        "        # or experinece with OOM issue, you can set a smaller batch size\n",
        "        self.batch_size = 50\n",
        "        # The epochs of the exps. The referenced model achieve over 95% test accuracy after 1 epoch.\n",
        "        self.epochs = 1\n",
        "        # The learning rate of the SGD optimizer\n",
        "        self.lr = 3e-4 #0.1\n",
        "        # The momentum of SGD optimizer\n",
        "        self.momentum = 0.5\n",
        "        # how many iterations to display the training stats\n",
        "        self.log_interval = 10\n",
        "        # The height of input image\n",
        "        self.img_h = 28\n",
        "        # The width of the input image\n",
        "        self.img_w = 28\n",
        "        # The length of input sequence\n",
        "        self.input_seq_len = 5\n",
        "        # The lenght of the sequence to predict\n",
        "        self.target_seq_len = 2\n",
        "        # The list to sample shift to generate the training sequence.\n",
        "        self.train_shift_list = [1,2,4,5]\n",
        "        # The list to sample shift to generate the testing sequence. \n",
        "        self.test_shift_list = [1,2,3,4,5]"
      ],
      "id": "14e6af01",
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79b9552c-9f53-4a5e-97d9-3b3c8c5c817a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d02f8e43-e484-4b53-9304-9ea6e74e1d76"
      },
      "source": [
        "# pytorch mnist cnn + lstm\n",
        "# Load necessary library\n",
        "\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.dataset import Dataset \n",
        "from torch.autograd import Variable\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt \n",
        "args = Args()\n",
        "torch.manual_seed(args.seed)"
      ],
      "id": "79b9552c-9f53-4a5e-97d9-3b3c8c5c817a",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb46e5811b0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71d87a29"
      },
      "source": [
        "### 0. The dataloader\n",
        "- Load the data from csv file\n",
        "- Generate the input and target image sequences spaced by constant distance sampled from the predefined shift list"
      ],
      "id": "71d87a29"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "713c2177-153d-446b-ae15-9489c0bba936"
      },
      "source": [
        "# The dataset to generated training and testing sequence\n",
        "class MNIST_SEQ_DATASET(Dataset):\n",
        "    def __init__(self, csv_path, height, width, input_len, output_len, seq_shift, transform=None):\n",
        "        \"\"\"\n",
        "        Custom dataset example for reading data from csv\n",
        "\n",
        "        Args:\n",
        "            csv_path (string): path to csv file\n",
        "            height (int): image height\n",
        "            width (int): image width\n",
        "            transform: pytorch transforms for transforms and tensor conversion\n",
        "        \"\"\"\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.labels = np.asarray(self.data.iloc[:, 0])\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.input_len = input_len\n",
        "        self.output_len = output_len\n",
        "        self.transform = transform\n",
        "        self.seq_shift = seq_shift\n",
        "        unique_label_array = np.unique(self.labels)\n",
        "        self.label_data_id_dict = {}\n",
        "        for unique_label in unique_label_array:\n",
        "            self.label_data_id_dict[unique_label] = np.where(self.labels == unique_label)[0]   \n",
        "        \n",
        "    def get_single_image(self, index):\n",
        "        single_image_label = self.labels[index]\n",
        "        # Read each 784 pixels and reshape the 1D array ([784]) to 2D array ([28,28])\n",
        "        img_as_np = np.asarray(self.data.iloc[index][1:]).reshape(28, 28).astype('uint8')\n",
        "        # Convert image from numpy array to PIL image, mode 'L' is for grayscale\n",
        "        img_as_img = Image.fromarray(img_as_np)\n",
        "        img_as_img = img_as_img.convert('L')\n",
        "        # Transform image to tensor\n",
        "        if self.transform is not None:\n",
        "            img_as_tensor = self.transform(img_as_img)\n",
        "        # Return image and the label\n",
        "        return (img_as_tensor, single_image_label)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # Randomly sample the shift from predefined shift list\n",
        "        seq_shift = np.random.choice(self.seq_shift)\n",
        "        # Randomly select one category as leading digit\n",
        "        start_idx = np.random.choice(10)\n",
        "        # The sequence with following digits\n",
        "        seq_digit = np.arange(start_idx, start_idx + seq_shift * (self.input_len + self.output_len), seq_shift)\n",
        "        # Modulo opeartion over the digit sequence\n",
        "        seq_digit = seq_digit % 10\n",
        "        img_seq = []\n",
        "        label_seq = []\n",
        "        # Collect the images for each digit\n",
        "        for digit in seq_digit:\n",
        "            data_id = np.random.choice(self.label_data_id_dict[digit])\n",
        "            img, label = self.get_single_image(data_id)\n",
        "            img_seq.append(img)\n",
        "            label_seq.append(label)\n",
        "        # Return image and the label\n",
        "        input_img_seq = img_seq[:self.input_len]\n",
        "        input_label_seq = label_seq[:self.input_len]\n",
        "        target_img_seq = img_seq[self.input_len:]\n",
        "        target_label_seq = label_seq[self.input_len:]\n",
        "        return torch.stack(input_img_seq), torch.stack(target_img_seq), \\\n",
        "                torch.from_numpy(np.stack(input_label_seq)), \\\n",
        "                torch.from_numpy(np.stack(target_label_seq)), \\\n",
        "                seq_shift\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data.index)"
      ],
      "id": "713c2177-153d-446b-ae15-9489c0bba936",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9e8b05a"
      },
      "source": [
        "# Instantiate the dataset which is then wrapperd by the DataLoader for effective prefecting\n",
        "train_path = '/content/drive/MyDrive/Colab Notebooks/mnist_train.csv'\n",
        "test_path = '/content/drive/MyDrive/Colab Notebooks/mnist_test.csv'\n",
        "transformations = transforms.Compose([transforms.ToTensor()])\n",
        "mnist_train = \\\n",
        "    MNIST_SEQ_DATASET(train_path,\n",
        "                             args.img_h, args.img_w, args.input_seq_len, args.target_seq_len,\n",
        "                             args.test_shift_list,\n",
        "                             transformations)\n",
        "\n",
        "mnist_test = \\\n",
        "    MNIST_SEQ_DATASET(test_path,\n",
        "                             args.img_h, args.img_w, args.input_seq_len, args.target_seq_len,\n",
        "                             args.test_shift_list,\n",
        "                             transformations)\n",
        "\n",
        "mnist_train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                                    batch_size=args.batch_size,\n",
        "                                                    shuffle=True)\n",
        "\n",
        "mnist_test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
        "                                                    batch_size=args.batch_size,\n",
        "                                                    shuffle=False)"
      ],
      "id": "d9e8b05a",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "354edd11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "e80ba7f4-79d5-4c73-dbcc-e84c5f628f25"
      },
      "source": [
        "# Display the input sequence and target sequence\n",
        "input_img_seq, target_img_seq, input_label_seq, target_label_seq, seq_shift = mnist_train[0]\n",
        "img_to_disp = input_img_seq.permute(1,2,0,3).reshape(args.img_h,-1,args.img_w)\n",
        "input_img_seq = img_to_disp.reshape(args.img_h, -1)\n",
        "img_to_disp = target_img_seq.permute(1,2,0,3).reshape(args.img_h,-1,args.img_w)\n",
        "target_img_seq = img_to_disp.reshape(args.img_h, -1)\n",
        "plt.imshow(input_img_seq,  cmap=\"gray\")\n",
        "plt.title('input sequence shifed by {}'.format(seq_shift))\n",
        "plt.show()\n",
        "plt.imshow(target_img_seq, cmap=\"gray\")\n",
        "plt.title('target sequence shifed by {}'.format(seq_shift))\n",
        "plt.show()"
      ],
      "id": "354edd11",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd654469"
      },
      "source": [
        "### 1. (TODO) The CNN-LSTM Model [20 points]\n",
        "- Complete the following section to create a CNN-LSTM model for sequence predicting problem. \n",
        "- You can borrow the CNN design from previous section as CNN encodes the categorical features of image\n",
        "- The CNN-LSTM should consist of the three modules: \n",
        "    - CNN for extracting visual features to a single feature vector\n",
        "    - LSTM taking as input the sequence of feature vector from CNN and producing the hidden feature to predict the next element\n",
        "    - A decoder to convert the LSTM prediction to categorical distribution"
      ],
      "id": "cd654469"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S66F7UIl4qL"
      },
      "source": [
        "#Attempt 1, please look at both, though I think this one is closer to correct.\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"Custom CNN model to extract visual features from input image\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\" Define and instantiate your layers\"\"\"\n",
        "        super(CNN, self).__init__()\n",
        "        # YOUR CODE HERE\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 5, 1, 2),                              \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(2),    \n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(3136, 32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" \n",
        "        Run forward pass on input image X\n",
        "        \n",
        "        Args:\n",
        "            x: torch tensor of input image, \n",
        "                with shape of [batch_size, 1, img_h, img_w]\n",
        "        \n",
        "        Return:\n",
        "            out: torch tensor of feature vector computed on input image, \n",
        "                with shape of [batch_size, latent_dim]\n",
        "         \n",
        "        \"\"\"\n",
        "        x = self.layers(x)\n",
        "        x = x.view(x.size(0), -1)     \n",
        "        output = self.fc(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "class CNN_LSTM(nn.Module):\n",
        "    \"\"\" Custom CNN-LSTM model for sequence prediction problem \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\" Define and instantiate your layers\"\"\"\n",
        "        super(CNN_LSTM, self).__init__()\n",
        "        # YOUR CODE HERE\n",
        "        self.hidden_size = 32\n",
        "        self.num_layers = 1\n",
        "        self.input_size = 32\n",
        "\n",
        "        self.cnn = CNN()\n",
        "        self.cnn = self.cnn.to(args.device)\n",
        "\n",
        "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(self.hidden_size, 10) \n",
        "\n",
        "    def forward(self, x, num_step_to_predict):\n",
        "        \"\"\" \n",
        "        Run forward pass on image squence x and predict the future digitss\n",
        "        \n",
        "        Args:\n",
        "            x : torch tensor of input image sequence, \n",
        "                    with shape of [batch_size, input_time_step, 1, img_h, img_w]\n",
        "            num_step_to_predict: an interger on how many steps to predict. \n",
        "            \n",
        "        Returns:\n",
        "            output: torch tensor of predicted categorical distribution  \n",
        "                    for the ENTIRE sequence, including input and predicted sequence, \n",
        "                    with shape of [batch_size, input_time_step + num_step_to_predict, 10].\n",
        "                    Noted the output from i step is the prediction for 1+1 step. \n",
        "            \n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        x = x.view(250,1,28,28)\n",
        "        #print(x.shape)\n",
        "        out = self.cnn(x) \n",
        "        new_out = out.view(50,5,32)\n",
        "\n",
        "        #For testing without cnn fc layer (output [250,x,32])...dead end\n",
        "        #test_x = out.view(out.size(0), out.size(1), -1)\n",
        "        #test_x = test_x.permute(0, 2, 1)        \n",
        "        #h0 = Variable(torch.zeros(self.num_layers, 250, self.hidden_size)).to(args.device) \n",
        "        #c0 = Variable(torch.zeros(self.num_layers, 250, self.hidden_size)).to(args.device)\n",
        "\n",
        "        h0 = Variable(torch.zeros(self.num_layers, 50, self.hidden_size)).to(args.device) \n",
        "        c0 = Variable(torch.zeros(self.num_layers, 50, self.hidden_size)).to(args.device)\n",
        "\n",
        "        test_x, (h0, c0) = self.lstm(new_out, (h0, c0))\n",
        "        \n",
        "        #For testing without cnn fc layer (output [250,x,32])...dead end\n",
        "        #test_x = test_x[:, -1, :]\n",
        "        #my_tensor = test_x\n",
        "        #test_x = test_x.view(50,5,32)\n",
        "\n",
        "        #For testing without cnn fc layer (output [250,x,32])...dead end\n",
        "        #h0 = Variable(torch.zeros(self.num_layers, 50, self.hidden_size)).to(args.device) \n",
        "        #c0 = Variable(torch.zeros(self.num_layers, 50, self.hidden_size)).to(args.device)\n",
        "        for i in range(num_step_to_predict):\n",
        "          test_x, (h0, c0) = self.lstm(test_x, (h0, c0))\n",
        "          save_x = test_x[:, -1, :]\n",
        "\n",
        "          #For testing without cnn fc layer (output [250,x,32])...dead end\n",
        "          #my_tensor = torch.cat([my_tensor, save_x], dim=0)\n",
        "\n",
        "          out = torch.cat([out, save_x], dim=0)\n",
        "\n",
        "          #to save out only future sequence elements \n",
        "          #if i == 0:\n",
        "          #  my_tensor = save_x\n",
        "          #else:\n",
        "          #  my_tensor = torch.cat([my_tensor, save_x], dim=0)\n",
        "\n",
        "        #print(int_tensor.shape)\n",
        "        fc_final_out = self.fc(out)\n",
        "        #print(fc_final_out.shape)\n",
        "        fc_final_out = fc_final_out.view(50,7,10)\n",
        "\n",
        "        return fc_final_out"
      ],
      "id": "4S66F7UIl4qL",
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "879a59ad"
      },
      "source": [
        "#Attempt 2\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"Custom CNN model to extract visual features from input image\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\" Define and instantiate your layers\"\"\"\n",
        "        super(CNN, self).__init__()\n",
        "        # YOUR CODE HERE\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 5, 1, 2),                              \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(2),    \n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(3136, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" \n",
        "        Run forward pass on input image X\n",
        "        \n",
        "        Args:\n",
        "            x: torch tensor of input image, \n",
        "                with shape of [batch_size, 1, img_h, img_w]\n",
        "        \n",
        "        Return:\n",
        "            out: torch tensor of feature vector computed on input image, \n",
        "                with shape of [batch_size, latent_dim]\n",
        "         \n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        x = self.layers(x)\n",
        "        x = x.view(x.size(0), -1)       \n",
        "        output = self.fc(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "class CNN_LSTM(nn.Module):\n",
        "    \"\"\" Custom CNN-LSTM model for sequence prediction problem \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\" Define and instantiate your layers\"\"\"\n",
        "        super(CNN_LSTM, self).__init__()\n",
        "        # YOUR CODE HERE\n",
        "        self.hidden_size = 10\n",
        "        self.num_layers = 1\n",
        "        self.input_size = 10\n",
        "\n",
        "        self.cnn = CNN()\n",
        "        self.cnn = self.cnn.to(args.device)\n",
        "\n",
        "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(self.hidden_size, 10)\n",
        "\n",
        "    def forward(self, x, num_step_to_predict):\n",
        "        \"\"\" \n",
        "        Run forward pass on image squence x and predict the future digitss\n",
        "        \n",
        "        Args:\n",
        "            x : torch tensor of input image sequence, \n",
        "                    with shape of [batch_size, input_time_step, 1, img_h, img_w]\n",
        "            num_step_to_predict: an interger on how many steps to predict. \n",
        "            \n",
        "        Returns:\n",
        "            output: torch tensor of predicted categorical distribution  \n",
        "                    for the ENTIRE sequence, including input and predicted sequence, \n",
        "                    with shape of [batch_size, input_time_step + num_step_to_predict, 10].\n",
        "                    Noted the output from i step is the prediction for 1+1 step. \n",
        "            \n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        x = torch.transpose(x, 0, 1)\n",
        "\n",
        "        for i in range(args.input_seq_len): \n",
        "          cnn_out = self.cnn(x[i])\n",
        "          cnn_out = cnn_out.unsqueeze(1)\n",
        "          if i == 1:       \n",
        "            my_tensor = torch.cat([last_x, cnn_out], dim=1)\n",
        "          elif i > 1:\n",
        "            my_tensor = torch.cat([my_tensor, cnn_out], dim=1)\n",
        "          else:\n",
        "            last_x = cnn_out\n",
        "\n",
        "        out = my_tensor\n",
        "        h0 = torch.zeros(self.num_layers, args.batch_size, self.hidden_size).to(args.device) \n",
        "        c0 = torch.zeros(self.num_layers, args.batch_size, self.hidden_size).to(args.device)\n",
        "        for i in range(num_step_to_predict):\n",
        "          out, (h0, c0) = self.lstm(out, (h0.detach(), c0.detach())) \n",
        "          save_out = self.fc(out[:, -1, :])\n",
        "          save_out = save_out.unsqueeze(1)\n",
        "          my_tensor = torch.cat([my_tensor, save_out], dim=1)\n",
        "\n",
        "        return my_tensor"
      ],
      "id": "879a59ad",
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36ee3154"
      },
      "source": [
        "### 2. (TODO) The Training Loop [15 points]\n",
        "- Instantiate the model and optimizer\n",
        "- Select proper loss function for this task\n",
        "- Complete the training loop"
      ],
      "id": "36ee3154"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "064084e1-ac41-4e84-a666-a3446b69fe9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1d81e501-92a7-44fc-a60a-3ff108791d65"
      },
      "source": [
        "model = CNN_LSTM()\n",
        "model = model.to(args.device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "loss_func = nn.NLLLoss()\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (input_img_seq, target_img_seq, input_label_seq, target_label_seq, seq_shift) in enumerate(mnist_train_loader):\n",
        "        # batch_size * input_seq_len * 1 * img_h * img_w\n",
        "        input_img_seq = input_img_seq.to(args.device)\n",
        "        #print(input_img_seq.shape)\n",
        "        # batch_size * input_seq_len\n",
        "        input_label_seq = input_label_seq.to(args.device)\n",
        "        #print(input_label_seq)\n",
        "        # batch_size * output_seq_len * 1 * img_h * img_w\n",
        "        target_img_seq = target_img_seq.to(args.device)\n",
        "        # batch_size * output_seq_len\n",
        "        target_label_seq = target_label_seq.to(args.device)\n",
        "        \n",
        "        # YOUR CODE HERE \n",
        "        outputs = model(input_img_seq, args.target_seq_len)\n",
        "\n",
        "        labels = torch.cat([input_label_seq, target_label_seq], dim=1)\n",
        "\n",
        "        outputs = torch.transpose(outputs, 2, 1)\n",
        "        loss = loss_func(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:       \n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            outputs = model(input_img_seq,2)\n",
        "            predicted = torch.max(outputs.data, 2)\n",
        "\n",
        "            total += target_label_seq.size(0)\n",
        "\n",
        "            p = predicted.indices.cpu().detach().numpy()\n",
        "            l = labels.cpu().detach().numpy()\n",
        "            for i in range(len(p)):\n",
        "                #print(p[i],l[i])\n",
        "                if (p[i] == l[i]).all():\n",
        "                    correct = correct + 1\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            print('batch_idx: {}. accuracy: {}'.format(batch_idx, accuracy))\n",
        "        \n",
        "for epoch in range(args.epochs):\n",
        "    train(epoch)"
      ],
      "id": "064084e1-ac41-4e84-a666-a3446b69fe9a",
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_idx: 0. accuracy: 0.0\n",
            "batch_idx: 100. accuracy: 0.0\n",
            "batch_idx: 200. accuracy: 0.0\n",
            "batch_idx: 300. accuracy: 0.0\n",
            "batch_idx: 400. accuracy: 0.0\n",
            "batch_idx: 500. accuracy: 0.0\n",
            "batch_idx: 600. accuracy: 0.0\n",
            "batch_idx: 700. accuracy: 0.0\n",
            "batch_idx: 800. accuracy: 0.0\n",
            "batch_idx: 900. accuracy: 0.0\n",
            "batch_idx: 1000. accuracy: 0.0\n",
            "batch_idx: 1100. accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2178a45"
      },
      "source": [
        "## 3. Test \n",
        "- Once your model achieve descent training accuracy, you can run test to validate your model\n",
        "- You should achieve at least 93% Top1 Acc to get full credit."
      ],
      "id": "e2178a45"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60c090d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b3dfacb4-7cad-45ef-97a5-9f7d0ddad6e7"
      },
      "source": [
        "def test():\n",
        "    model.eval()\n",
        "    top1_acc_dict = {test_shift:{'sum_acc':0, 'count':0} for test_shift in args.test_shift_list}\n",
        "    top5_acc_dict = {test_shift:{'sum_acc':0, 'count':0} for test_shift in args.test_shift_list}\n",
        "    for batch_idx, (input_img_seq, target_img_seq, input_label_seq, target_label_seq, seq_shift) in enumerate(mnist_test_loader):\n",
        "        batch_size = input_img_seq.shape[0]\n",
        "        # batch_size * input_seq_len * 1 * img_h * img_w\n",
        "        input_img_seq = input_img_seq.to(args.device)\n",
        "        # batch_size * input_seq_len\n",
        "        input_label_seq = input_label_seq.to(args.device)\n",
        "        # batch_size * output_seq_len * 1 * img_h * img_w\n",
        "        target_img_seq = target_img_seq.to(args.device)\n",
        "        # batch_size * output_seq_len\n",
        "        target_label_seq = target_label_seq.to(args.device)\n",
        "        \n",
        "        total_pred = model(input_img_seq, args.target_seq_len)\n",
        "        pred = total_pred[:,:-1][:,-1 * args.target_seq_len:].reshape(-1,10)\n",
        "\n",
        "        _, top_index = pred.topk(5, dim = -1)\n",
        "        correct_pred = top_index == target_label_seq.reshape(-1)[:,None]\n",
        "        top1_acc = correct_pred[:,0].float().reshape(batch_size, -1) * 100\n",
        "        top5_acc = correct_pred[:,:5].sum(dim = -1).float().reshape(batch_size, -1) * 100\n",
        "        for seq_shift_ele in torch.unique(seq_shift):\n",
        "            top1_acc_val = top1_acc[torch.where(seq_shift == seq_shift_ele)[0]].mean(dim = -1).sum()\n",
        "            top1_acc_count = torch.where(seq_shift == seq_shift_ele)[0].shape[0]\n",
        "            top1_acc_dict[seq_shift_ele.item()]['sum_acc'] += top1_acc_val.item()\n",
        "            top1_acc_dict[seq_shift_ele.item()]['count'] += top1_acc_count\n",
        "            \n",
        "            top5_acc_val = top5_acc[torch.where(seq_shift == seq_shift_ele)[0]].mean(dim = -1).sum()\n",
        "            top5_acc_count = torch.where(seq_shift == seq_shift_ele)[0].shape[0]\n",
        "            top5_acc_dict[seq_shift_ele.item()]['sum_acc'] += top5_acc_val.item()\n",
        "            top5_acc_dict[seq_shift_ele.item()]['count'] += top5_acc_count \n",
        "\n",
        "        total_top1_acc = np.mean(np.stack([val['sum_acc'] / (val['count'] + 1e-5) for key, val in top1_acc_dict.items()]))\n",
        "        total_top5_acc = np.mean(np.stack([val['sum_acc'] / (val['count'] + 1e-5) for key, val in top5_acc_dict.items()]))\n",
        "        \n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Test: [{}/{} ({:.0f}%)] Top1 Acc: {:1f}, Top5 Acc: {:1f}'.format(\n",
        "                batch_idx * input_img_seq.shape[0], len(mnist_test_loader.dataset),\n",
        "                100. * batch_idx * input_img_seq.shape[0] / len(mnist_test_loader.dataset), total_top1_acc, total_top5_acc))\n",
        "        \n",
        "    top1_acc_each_shift = np.stack([val['sum_acc'] / (val['count'] + 1e-5) for key, val in top1_acc_dict.items()])\n",
        "    top5_acc_each_shift = np.stack([val['sum_acc'] / (val['count'] + 1e-5) for key, val in top1_acc_dict.items()])\n",
        "    for idx, (key, _) in enumerate(top1_acc_dict.items()):\n",
        "        print('Shift {}, Test Top1 Acc: {:1f}, Test Top5 Acc: {:1f}'.format(key, top1_acc_each_shift[idx], top5_acc_each_shift[idx]))\n",
        "test()"
      ],
      "id": "60c090d4",
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test: [0/10000 (0%)] Top1 Acc: 8.833325, Top5 Acc: 44.999954\n",
            "Test: [500/10000 (5%)] Top1 Acc: 10.201686, Top5 Acc: 51.382256\n",
            "Test: [1000/10000 (10%)] Top1 Acc: 10.189020, Top5 Acc: 51.564803\n",
            "Test: [1500/10000 (15%)] Top1 Acc: 10.461947, Top5 Acc: 51.576704\n",
            "Test: [2000/10000 (20%)] Top1 Acc: 10.240449, Top5 Acc: 50.720489\n",
            "Test: [2500/10000 (25%)] Top1 Acc: 10.364292, Top5 Acc: 51.097089\n",
            "Test: [3000/10000 (30%)] Top1 Acc: 10.351687, Top5 Acc: 50.576751\n",
            "Test: [3500/10000 (35%)] Top1 Acc: 10.390537, Top5 Acc: 50.290246\n",
            "Test: [4000/10000 (40%)] Top1 Acc: 10.454577, Top5 Acc: 50.376070\n",
            "Test: [4500/10000 (45%)] Top1 Acc: 10.383560, Top5 Acc: 50.442450\n",
            "Test: [5000/10000 (50%)] Top1 Acc: 10.393518, Top5 Acc: 50.530966\n",
            "Test: [5500/10000 (55%)] Top1 Acc: 10.431199, Top5 Acc: 50.485201\n",
            "Test: [6000/10000 (60%)] Top1 Acc: 10.347662, Top5 Acc: 50.318645\n",
            "Test: [6500/10000 (65%)] Top1 Acc: 10.282665, Top5 Acc: 50.233260\n",
            "Test: [7000/10000 (70%)] Top1 Acc: 10.220241, Top5 Acc: 50.215091\n",
            "Test: [7500/10000 (75%)] Top1 Acc: 10.195473, Top5 Acc: 50.158270\n",
            "Test: [8000/10000 (80%)] Top1 Acc: 10.203756, Top5 Acc: 50.204876\n",
            "Test: [8500/10000 (85%)] Top1 Acc: 10.226572, Top5 Acc: 50.222269\n",
            "Test: [9000/10000 (90%)] Top1 Acc: 10.220708, Top5 Acc: 50.149175\n",
            "Test: [9500/10000 (95%)] Top1 Acc: 10.293320, Top5 Acc: 50.142142\n",
            "Shift 1, Test Top1 Acc: 10.259740, Test Top5 Acc: 10.259740\n",
            "Shift 2, Test Top1 Acc: 10.040775, Test Top5 Acc: 10.040775\n",
            "Shift 3, Test Top1 Acc: 10.702179, Test Top5 Acc: 10.702179\n",
            "Shift 4, Test Top1 Acc: 10.371820, Test Top5 Acc: 10.371820\n",
            "Shift 5, Test Top1 Acc: 10.079840, Test Top5 Acc: 10.079840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T25vwV3Ng2Ge"
      },
      "source": [
        ""
      ],
      "id": "T25vwV3Ng2Ge",
      "execution_count": null,
      "outputs": []
    }
  ]
}